{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b216e6d3",
   "metadata": {},
   "source": [
    "# Facemask Detection Model Documentation\n",
    "This notebook documents the workflow for building, training, evaluating, and deploying a facemask detection model using a Convolutional Neural Network (CNN) in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696f047",
   "metadata": {},
   "source": [
    "## 1. Introduction & Overview\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of our CNN-based face mask detection model. We'll explore the dataset, understand the model architecture, analyze the training process, and evaluate performance.\n",
    "\n",
    "**What you'll learn**\n",
    "\n",
    "- Dataset characteristics and preprocessing\n",
    "- CNN architecture design choices\n",
    "- Training methodology and optimization\n",
    "- Model performance evaluation and analysis\n",
    "\n",
    "**Project Structure:**\n",
    "\n",
    "- `src/`: Source code for data handling, training, evaluation, and live detection\n",
    "- `models/`: Model definition and saved weights\n",
    "- `data/`: Organized dataset (with_mask/without_mask)\n",
    "- `notebooks/`: Documentation and demo notebooks\n",
    "- `requirements.txt`: Dependencies\n",
    "- `README.md`: Project instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65603c86",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration\n",
    "\n",
    "We use the [Face-Mask-Detection Dataset](https://huggingface.co/datasets/DamarJati/Face-Mask-Detection) from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bd5f6",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d551602",
   "metadata": {},
   "source": [
    "Firstly, we will run the following code block to import libraries necessary for analyzing the dataset and running the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52a445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Swarit Kumar Singh\\OneDrive\\Desktop\\Projects\\Facemask-Detection\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset                   # loads the dataset\n",
    "import matplotlib.pyplot as plt                     # for plotting\n",
    "import random                                       # for selecting random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c55a2",
   "metadata": {},
   "source": [
    "After importing the libaries, we will load the dataset and then inspect it's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928de00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 11792\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"DamarJati/Face-Mask-Detection\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478f18c",
   "metadata": {},
   "source": [
    "From this, we can see that the dataset in question has two features:\n",
    "\n",
    "- 'image'\n",
    "- 'label'\n",
    "\n",
    "Additionally, we can see that it has 11792 rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f34318",
   "metadata": {},
   "source": [
    "To further gauge the data, we will look at the very first sample in the dataset by running the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda4dbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=36x36 at 0x25684A87E00>, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f0d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample[\"image\"].show()\n",
    "print(\"Label:\", sample[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce857d",
   "metadata": {},
   "source": [
    "This tells us that the first image has a label of **0**, meaning that it is an image of an individual with a mask on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b069dd4",
   "metadata": {},
   "source": [
    "We can further view 5 random samples from the dataset by running the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d88cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(dataset[\"train\"])), 5)\n",
    "samples = [dataset[\"train\"][i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bbb241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: without_mask\n",
      "Label: with_mask\n",
      "Label: with_mask\n",
      "Label: with_mask\n",
      "Label: without_mask\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"with_mask\", 1: \"without_mask\"}\n",
    "\n",
    "for s in samples:\n",
    "    s[\"image\"].show()\n",
    "    print(\"Label:\", label_map[s[\"label\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad5db0",
   "metadata": {},
   "source": [
    "Our random image search leads us to find images that are in both classes (in some cases, the code might run and show images from a singular class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e409b2e",
   "metadata": {},
   "source": [
    "We can run the following lines of code to plot the data (prior to cleaning), and visualize what we're looking at here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d1e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(dataset[\"train\"])), 100)\n",
    "subset = dataset[\"train\"].select(indices)\n",
    "labels = [example[\"label\"] for example in subset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07bbeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels, bins=2, edgecolor='black')\n",
    "plt.xticks([0, 1], [\"with_mask\", \"without_mask\"])\n",
    "plt.title(\"Class Distribution (100 Random Samples)\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c3c24",
   "metadata": {},
   "source": [
    "Through the plot, we can see a fairly even distribution of class magnitude (with_mask and without_mask). A few things to note:\n",
    "- Plot was ran with 100 random samples (out of approximately 12000) to save time\n",
    "- Plot may differ with each run of randomizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a4917c",
   "metadata": {},
   "source": [
    "Another thing is that, if it wasn't obvious before, it becomes immediately obvious that this is a binary classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5204b6",
   "metadata": {},
   "source": [
    "## 3. Preparing the Dataset for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1e38a",
   "metadata": {},
   "source": [
    "First, we will call 'dataset' to see if splits within the data already exist for 'train' and 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f598c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 11792\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e04ba",
   "metadata": {},
   "source": [
    "Since there's only a 'train' split, we will create the test split ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740eff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59547117",
   "metadata": {},
   "source": [
    "This splits the model into: \n",
    "- 80% for Training\n",
    "- 20% for Testing (Splitting this again later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5841e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2deb8",
   "metadata": {},
   "source": [
    "This has split the testing split evenly into a test and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c347a66",
   "metadata": {},
   "source": [
    "Finally, we will combine the splits into a complete dataset using a DatasetDict, which will allow us to organize our splits into a clean structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93699d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"valid\": val_test[\"train\"],\n",
    "    \"test\": val_test[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d47893c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 9433\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1179\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1180\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149e01e",
   "metadata": {},
   "source": [
    "Our inspection shows us the number of values in each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57069e0b",
   "metadata": {},
   "source": [
    "### Image Preprocessing\n",
    "We will now resize all images to the same shape to prepare them for model training.\n",
    "Many standard deep learning models expect inputs of shape (224, 224).  \n",
    "We will also convert all images to NumPy arrays for compatibility with PyTorch or TensorFlow later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a220ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image                   # lets us resize images\n",
    "import numpy as np                      # used to convert images to arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d3b90",
   "metadata": {},
   "source": [
    "For the purposes of the demo in this Jupyter notebook, we will shrink our splits to 100 values each to speed things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e63e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in final_dataset:\n",
    "    final_dataset[split] = final_dataset[split].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7fd0a",
   "metadata": {},
   "source": [
    "Firstly, we will define a pre-processing function which:\n",
    "\n",
    "- Resizes each image to 224 x 224 pixels\n",
    "- Converts the image to a NumPy array\n",
    "- Returns the modified example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc828d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    image = example[\"image\"].resize((224, 224))\n",
    "    example[\"image\"] = np.array(image)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc125b10",
   "metadata": {},
   "source": [
    "We will now apply the pre-processing to our train datasplit (it will take to long to run it on all of the splits, so we will run it on a random sample of 100 images from our train datasplit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03df5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset[\"train\"] = final_dataset[\"train\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9763649",
   "metadata": {},
   "source": [
    "🧠 4. Modeling with PyTorch\n",
    "\n",
    "We will now build a simple image classification model using PyTorch.  \n",
    "Our goal is to classify images as either **\"with_mask\"** or **\"without_mask\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c490d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):                        # Ultimately, the CNN is an object\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)            # first convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)           # second convolutional layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8836e",
   "metadata": {},
   "source": [
    "Next, we will convert out Hugging Face dataset to a PyTorch format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4a3eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.images = hf_dataset[\"image\"]\n",
    "        self.labels = hf_dataset[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].resize((224, 224))     # force resize now\n",
    "        img = torch.tensor(np.array(img), dtype=torch.float32).permute(2, 0, 1)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b653d",
   "metadata": {},
   "source": [
    "- permute(2, 0, 1) converts images from (H, W, C) to (C, H, W), which PyTorch expects\n",
    "- This prepares each sample as (image_tensor, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f02ff",
   "metadata": {},
   "source": [
    "Following that, we will now create Data Loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbfefc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(MaskDataset(final_dataset[\"train\"]), batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(MaskDataset(final_dataset[\"valid\"]), batch_size=16)\n",
    "test_loader = DataLoader(MaskDataset(final_dataset[\"test\"]), batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42278621",
   "metadata": {},
   "source": [
    "- batch_size=16: processes 16 images at once\n",
    "- shuffle=True: only for training (to randomize batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b88d20",
   "metadata": {},
   "source": [
    "We will now set up our model, our optimizer, and also set our rate of loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0feffad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda0fad",
   "metadata": {},
   "source": [
    "- CNN() is the custom model\n",
    "\n",
    "- Adam is commonly used optimizer\n",
    "\n",
    "- CrossEntropyLoss is used for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029b56a",
   "metadata": {},
   "source": [
    "We will now set up a simple training loop for 1 epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a284fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training epoch complete.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"1 training epoch complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cfd0f",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "We will now evaluate the trained model's performance on the validation and test sets.\n",
    "Accuracy is calculated by comparing predicted labels with true labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbac07f",
   "metadata": {},
   "source": [
    "> ⚠️ **Note:**  \n",
    "> The model was trained and evaluated on a reduced dataset (100 samples per split) for faster prototyping, therefore, the accuracy given below is exceedingly low.\n",
    "> When trained on the **full dataset**, the model achieves a validation accuracy of approximately **99%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa89e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 67.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521e635",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "To improve this project further:\n",
    "\n",
    "- Train on the **full dataset** (not just 100 samples) for much higher accuracy.\n",
    "- Add **image normalization** (e.g., scaling pixel values from 0–255 to 0–1).\n",
    "- Implement **data augmentation** to make the model more robust.\n",
    "- Track training and validation loss over multiple epochs.\n",
    "- Save the trained model and test it on real-world webcam input (live detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e2c15",
   "metadata": {},
   "source": [
    "**A walkthrough of how I created the real-world live detection is outlined on the LiveDetectionDocumentation.ipynb file** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
